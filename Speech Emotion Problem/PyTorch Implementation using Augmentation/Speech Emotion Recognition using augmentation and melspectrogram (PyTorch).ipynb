{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1969)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(1969)\n",
    "\n",
    "\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import gc\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import GRU, Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, Conv3D, ConvLSTM2D,Conv1D,Activation,LSTM\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import delta\n",
    "from python_speech_features import logfbank\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import glob\n",
    "import torch\n",
    "from torch import *\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Cyclic Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=16000\n",
    "AUDIO_LENGTH=16000\n",
    "AUDIO_SR=16000\n",
    "AUDIO_NOISES=[]\n",
    "# Making Random noise files\n",
    "for file in ['meld/train/disgust/MEL_dia1005_utt13_negative_DIS.wav',  'meld/train/fear/MEL_dia133_utt15_negative_FEA.wav',   \n",
    "             'meld/train/happy/MEL_dia95_utt14_positive_HAP.wav', 'meld/train/neutral/MEL_dia96_utt19_neutral_NEU.wav',\n",
    "             'meld/train/sad/MEL_dia124_utt12_negative_SAD.wav',  'meld/train/sad/MEL_dia148_utt0_negative_SAD.wav']:\n",
    "    #audio_file = AUDIO_DIR + '/train/audio/_background_noise_/' + file\n",
    "    wave = librosa.core.load(file, sr=AUDIO_SR)[0]\n",
    "    AUDIO_NOISES.append(wave)\n",
    "\n",
    "def custom_fft(y, fs):\n",
    "    T = 1.0 / fs\n",
    "    N = y.shape[0]\n",
    "    yf = fft(y)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "    # FFT is simmetrical, so we take just the first half\n",
    "    # FFT is also complex, to we take just the real part (abs)\n",
    "    vals = 2.0/N * np.abs(yf[0:N//2])\n",
    "    return xf, vals\n",
    "\n",
    "# Defining log-specgram\n",
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "# Defining padding function\n",
    "def pad_audio(samples):\n",
    "    if len(samples) >= L: return samples\n",
    "    else: return np.pad(samples, pad_width=(L - len(samples), 0), mode='constant', constant_values=(0, 0))\n",
    "    \n",
    "# Defining chopping function\n",
    "def chop_audio(samples, L=16000, num=20):\n",
    "    for i in range(num):\n",
    "        beg = np.random.randint(0, len(samples) - L)\n",
    "        yield samples[beg: beg + L]\n",
    "        \n",
    "# Defining spectrogram for wav file conversion\n",
    "def get_spectrogram(wav):\n",
    "    D = librosa.stft(wav, n_fft=480, hop_length=160,\n",
    "                     win_length=480, window='hamming')\n",
    "    spect, phase = librosa.magphase(D)\n",
    "    return spect\n",
    "\n",
    "# Defining funtion for adding random noise\n",
    "def tf_random_add_noise_transform(wave, noise_limit=0.2, u=0.5):\n",
    "\n",
    "    if random.random() < u:\n",
    "        num_noises = len(AUDIO_NOISES)\n",
    "        noise = AUDIO_NOISES[np.random.choice(num_noises)]\n",
    "\n",
    "        wave_length  = len(wave)\n",
    "        noise_length = len(noise)\n",
    "        p=noise_length - wave_length - 1\n",
    "        print(p)\n",
    "        t = np.random.randint(0, noise_length - wave_length - 1)\n",
    "        #t = np.random.randint(noise_length - wave_length - 1,0)\n",
    "        noise = noise[t:t + wave_length]\n",
    "\n",
    "        alpha = np.random.random() * noise_limit\n",
    "        wave  = np.clip(alpha * noise + wave, -1, 1)\n",
    "\n",
    "    return wave\n",
    "\n",
    "# Defining function for adding random time shift\n",
    "def tf_random_time_shift_transform(wave, shift_limit=0.2, u=0.5):\n",
    "    if random.random() < u:\n",
    "        wave_length  = len(wave)\n",
    "        shift_limit = shift_limit*wave_length\n",
    "        shift = np.random.randint(-shift_limit, shift_limit)\n",
    "        t0 = -min(0, shift)\n",
    "        t1 =  max(0, shift)\n",
    "        wave = np.pad(wave, (t0, t1), 'constant')\n",
    "        wave = wave[:-t0] if t0 else wave[t1:]\n",
    "\n",
    "    return wave\n",
    "\n",
    "# Defining function for adding random padding\n",
    "def tf_random_pad_transform(wave, length=AUDIO_LENGTH):\n",
    "\n",
    "    if len(wave)<AUDIO_LENGTH:\n",
    "        L = abs(len(wave)-AUDIO_LENGTH)\n",
    "        start = np.random.choice(L)\n",
    "        wave  = np.pad(wave, (start, L-start), 'constant')\n",
    "\n",
    "    elif len(wave)>AUDIO_LENGTH:\n",
    "        L = abs(len(wave)-AUDIO_LENGTH)\n",
    "        start = np.random.choice(L)\n",
    "        wave  = wave[start: start+AUDIO_LENGTH]\n",
    "\n",
    "    return wave\n",
    "\n",
    "# Defining function fixed padding (test file)\n",
    "def tf_fix_pad_transform(wave, length=AUDIO_LENGTH):\n",
    "    # wave = np.pad(wave, (0, max(0, AUDIO_LENGTH - len(wave))), 'constant')\n",
    "    # return wave\n",
    "\n",
    "    if len(wave)<AUDIO_LENGTH:\n",
    "        L = abs(len(wave)-AUDIO_LENGTH)\n",
    "        start = L//2\n",
    "        wave  = np.pad(wave, (start, L-start), 'constant')\n",
    "\n",
    "    elif len(wave)>AUDIO_LENGTH:\n",
    "        L = abs(len(wave)-AUDIO_LENGTH)\n",
    "        start = L//2\n",
    "        wave  = wave[start: start+AUDIO_LENGTH]\n",
    "\n",
    "    return wave\n",
    "\n",
    "\n",
    "def tf_random_scale_amplitude_transform(wave, scale_limit=0.1, u=0.5):\n",
    "    if random.random() < u:\n",
    "        scale = np.random.randint(-scale_limit, scale_limit)\n",
    "        wave = scale*wave\n",
    "    return wave\n",
    "\n",
    "# Defininig function for converting Wave to MFCC\n",
    "def tf_wave_to_mfcc(wave):\n",
    "\n",
    "    spectrogram = librosa.feature.melspectrogram(wave, sr=AUDIO_SR, n_mels=40, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
    "    #spectrogram = librosa.power_to_db(spectrogram)\n",
    "    idx = [spectrogram > 0]\n",
    "    spectrogram[idx] = np.log(spectrogram[idx])\n",
    "\n",
    "    dct_filters = librosa.filters.dct(n_filters=40, n_input=40)\n",
    "    mfcc = [np.matmul(dct_filters, x) for x in np.split(spectrogram, spectrogram.shape[1], axis=1)]\n",
    "    mfcc = np.hstack(mfcc)\n",
    "    mfcc = mfcc.astype(np.float32)\n",
    "\n",
    "    return mfcc\n",
    "\n",
    "# Defininig function for converting Wave to Melspectrogram\n",
    "def tf_wave_to_melspectrogram(wave):\n",
    "    spectrogram = librosa.feature.melspectrogram(wave, sr=AUDIO_SR, n_mels=40, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
    "    spectrogram = librosa.power_to_db(spectrogram)\n",
    "    spectrogram = spectrogram.astype(np.float32)\n",
    "\n",
    "    return spectrogram\n",
    "\n",
    "\n",
    "# Defininig function for converting Wave to combnation of Melspectrogram and MFCC\n",
    "def tf_wave_to_melspectrogram_mfcc(wave):\n",
    "\n",
    "    spectrogram = librosa.feature.melspectrogram(wave, sr=AUDIO_SR, n_mels=40, hop_length=160, n_fft=480, fmin=5, fmax=4500)\n",
    "    idx = [spectrogram > 0]\n",
    "    spectrogram[idx] = np.log(spectrogram[idx])\n",
    "\n",
    "    dct_filters = librosa.filters.dct(n_filters=40, n_input=40)\n",
    "    mfcc = [np.matmul(dct_filters, x) for x in np.split(spectrogram, spectrogram.shape[1], axis=1)]\n",
    "    mfcc = np.hstack(mfcc)\n",
    "    mfcc = mfcc.astype(np.float32)\n",
    "\n",
    "    spectrogram = librosa.power_to_db(spectrogram)\n",
    "    spectrogram = spectrogram.astype(np.float32)\n",
    "\n",
    "    all = np.concatenate((spectrogram[np.newaxis,:],mfcc[np.newaxis,:]))\n",
    "    return all\n",
    "\n",
    "# Defininig function for converting Wave to combnation of Log-Melspectrogram\n",
    "def tf_wave_to_melspectrogram1(wave):\n",
    "    spectrogram = librosa.feature.melspectrogram(wave, sr=AUDIO_SR, n_mels=40, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
    "    idx = [spectrogram > 0]\n",
    "    spectrogram[idx] = np.log(spectrogram[idx])\n",
    "    spectrogram = spectrogram.astype(np.float32)\n",
    "    return spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train time augmentation\n",
    "def train_augment(wave):\n",
    "    wave = tf_random_time_shift_transform(wave, shift_limit=0.2, u=0.5)\n",
    "#    wave = tf_random_add_noise_transform (wave, noise_limit=0.2, u=0.5)\n",
    "    wave = tf_random_pad_transform(wave)\n",
    "    tensor = tf_wave_to_melspectrogram(wave)[np.newaxis,:]\n",
    "    return tensor\n",
    "\n",
    "# Test time augmentation\n",
    "def valid_augment(wave):\n",
    "    wave = tf_fix_pad_transform(wave)\n",
    "    tensor = tf_wave_to_melspectrogram(wave)[np.newaxis,:]\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and validation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Label Encoding\n",
    "mappings={\"disgust\":int(0),\"fear\":int(1),\"happy\":int(2),\"neutral\":int(3),\"sad\":int(4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_rate = 16000\n",
    "y_train = []\n",
    "x_train = []\n",
    "mylist1=os.listdir('meld/train/')\n",
    "for file in mylist1:\n",
    "    mylist= os.listdir('meld/train/'+file+\"/\")\n",
    "    for index,y in enumerate(mylist):\n",
    "        samples, sample_rate = librosa.core.load('meld/train/'+file+\"/\"+y,mono=True,sr=16000)\n",
    "        # Using train augmentation\n",
    "        specgram=train_augment(samples)\n",
    "        x_train.append(specgram)\n",
    "        y_train.append(file)\n",
    "\n",
    "# Label Encoding target and converting train files to numpy array\n",
    "x_train = np.array(x_train)\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] in mappings:\n",
    "        y_train[i]=mappings[y_train[i]]\n",
    "y_train=np.array(y_train,dtype=\"int64\")\n",
    "x_train = torch.from_numpy(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new sampling\n",
    "new_sample_rate = 16000\n",
    "y_test = []\n",
    "x_test = []\n",
    "mylist1=os.listdir('meld/val/')\n",
    "for file in mylist1:\n",
    "    mylist= os.listdir('meld/val/'+file+\"/\")\n",
    "    for index,y in enumerate(mylist):\n",
    "        samples, sample_rate = librosa.core.load('meld/val/'+file+\"/\"+y,mono=True,sr=16000)\n",
    "        # Using test time augmentation\n",
    "        specgram=valid_augment(samples)\n",
    "        x_test.append(specgram)\n",
    "        y_test.append(file)\n",
    "        \n",
    "# Label Encoding target and converting valid files to numpy array\n",
    "x_test = np.array(x_test)\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] in mappings:\n",
    "        y_test[i]=mappings[y_test[i]]\n",
    "y_test=np.array(y_test,dtype=\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Basic Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cnn_Trad_Pool2_Net(nn.Module):\n",
    "    def __init__(self, in_shape=(1,40,101), num_classes=5 ):\n",
    "\n",
    "        super(Cnn_Trad_Pool2_Net, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1,  64, kernel_size=(8, 20), stride=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=(4, 10), stride=(1, 1))\n",
    "        self.fc = nn.Linear(26624,num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x,inplace=True)\n",
    "        x = F.max_pool2d(x,kernel_size=(2,2),stride=(2,2))\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x,inplace=True)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        #print(x.size())\n",
    "        x = F.dropout(x,p=0.5,training=self.training)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x  #logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Convnet with residual blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBn2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, dilation=1, stride=1, groups=1, is_bn=True):\n",
    "        super(ConvBn2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride, dilation=dilation, groups=groups, bias=False)\n",
    "        self.bn   = nn.BatchNorm2d(out_channels)\n",
    "        if is_bn is False:\n",
    "            self.bn =None\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class SeScale(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SeScale, self).__init__()\n",
    "        self.fc1 = nn.Conv2d(channel, reduction, kernel_size=1, padding=0)\n",
    "        self.fc2 = nn.Conv2d(reduction, channel, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.adaptive_avg_pool2d(x,1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, reduction=16):\n",
    "        super(ResBlock, self).__init__()\n",
    "        assert(in_planes==out_planes)\n",
    "\n",
    "        self.conv_bn1 = ConvBn2d(in_planes,  out_planes, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv_bn2 = ConvBn2d(out_planes, out_planes, kernel_size=3, padding=1, stride=1)\n",
    "        self.scale    = SeScale(out_planes, reduction)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z  = F.relu(self.conv_bn1(x),inplace=True)\n",
    "        z  = self.conv_bn2(z)\n",
    "        z  = self.scale(z)*z + x\n",
    "        z  = F.relu(z,inplace=True)\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "## net ##-------\n",
    "\n",
    "class SeResNet3(nn.Module):\n",
    "    def __init__(self, in_shape=(1,40,101), num_classes=5 ):\n",
    "        super(SeResNet3, self).__init__()\n",
    "        in_channels = in_shape[0]\n",
    "\n",
    "        self.layer1a = ConvBn2d(in_channels, 16, kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.layer1b = ResBlock( 16, 16)\n",
    "\n",
    "        self.layer2a = ConvBn2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.layer2b = ResBlock(32, 32)\n",
    "        self.layer2c = ResBlock(32, 32)\n",
    "\n",
    "        self.layer3a = ConvBn2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.layer3b = ResBlock(64, 64)\n",
    "        self.layer3c = ResBlock(64, 64)\n",
    "\n",
    "        self.layer4a = ConvBn2d( 64,128, kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.layer4b = ResBlock(128,128)\n",
    "        self.layer4c = ResBlock(128,128)\n",
    "\n",
    "        self.layer5a = ConvBn2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.layer5b = nn.Linear(256,256)\n",
    "\n",
    "        self.fc = nn.Linear(256,num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.layer1a(x),inplace=True)\n",
    "        x = self.layer1b(x)\n",
    "        x = F.max_pool2d(x,kernel_size=(2,2),stride=(2,2))\n",
    "\n",
    "        x = F.dropout(x,p=0.1,training=self.training)\n",
    "        x = F.relu(self.layer2a(x),inplace=True)\n",
    "        x = self.layer2b(x)\n",
    "        x = self.layer2c(x)\n",
    "        x = F.max_pool2d(x,kernel_size=(2,2),stride=(2,2))\n",
    "\n",
    "        x = F.dropout(x,p=0.2,training=self.training)\n",
    "        x = F.relu(self.layer3a(x),inplace=True)\n",
    "        x = self.layer3b(x)\n",
    "        x = self.layer3c(x)\n",
    "        x = F.max_pool2d(x,kernel_size=(2,2),stride=(2,2))\n",
    "\n",
    "        x = F.dropout(x,p=0.2,training=self.training)\n",
    "        x = F.relu(self.layer4a(x),inplace=True)\n",
    "        x = self.layer4b(x)\n",
    "        x = self.layer4c(x)\n",
    "\n",
    "        x = F.dropout(x,p=0.2,training=self.training)\n",
    "        x = F.relu(self.layer5a(x),inplace=True)\n",
    "        x = F.adaptive_avg_pool2d(x,1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.layer5b(x))\n",
    "\n",
    "        x = F.dropout(x,p=0.2,training=self.training)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x  #logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making object of the model\n",
    "H = 40\n",
    "W = 101\n",
    "#model = Cnn_Trad_Pool2_Net(in_shape=(1,H,W), num_classes=5)\n",
    "#model.cuda()\n",
    "model=SeResNet3(in_shape=(1,H,W), num_classes=5).cuda()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.dataset[index]\n",
    "\n",
    "        return data, target, index\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensor dataset\n",
    "x_train = torch.Tensor(x_train).cuda()\n",
    "x_test = torch.Tensor(x_test).cuda()\n",
    "y_train = torch.cuda.LongTensor(y_train).cuda()\n",
    "y_test = torch.cuda.LongTensor(y_test).cuda()\n",
    "\n",
    "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "valid = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "    \n",
    "train = MyDataset(train)\n",
    "valid = MyDataset(valid)\n",
    "\n",
    "# Creating train and validation loader objects\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining optimizer and scheduler for network \n",
    "from torch.optim.optimizer import Optimizer\n",
    "step_size = 300\n",
    "base_lr, max_lr = 0.001, 0.005\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                          lr=max_lr, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n",
    "               step_size=step_size, mode='exp_range',\n",
    "               gamma=0.99994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining loss function \n",
    "from torch.nn import *\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "#loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for epoch in range(20):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.  \n",
    "        for i, (x_batch, y_batch, index) in enumerate(train_loader):\n",
    "            \n",
    "            ################################################################################################            \n",
    "            y_pred = model(x_batch)\n",
    "            ################################################################################################\n",
    "            if scheduler:\n",
    "                scheduler.batch_step()\n",
    "            ###############################################################################################\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "\n",
    "            \n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "       \n",
    "        valid_preds_fold = np.zeros((x_test.size(0)))        \n",
    "        avg_val_loss = 0.\n",
    "        for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n",
    "            y_pred = model(x_batch).detach()\n",
    "            \n",
    "            avg_val_loss += criterion(y_pred,y_batch).item() / len(valid_loader)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1,20, avg_loss, avg_val_loss, elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model and loading it for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "modeltest = SeResNet3(in_shape=(1,H,W), num_classes=5).cuda()\n",
    "modeltest.load_state_dict(torch.load(\"model.pth\"))\n",
    "modeltest.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n",
    "    y_pred = modeltest(x_batch).detach()\n",
    "    prediction = torch.argmax(y_pred, dim=1)\n",
    "    \n",
    "    predictions.append(prediction.tolist())\n",
    "flattened_list = [y for x in predictions for y in x]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
