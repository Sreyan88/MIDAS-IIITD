{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1969)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(1969)\n",
    "\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import gc\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import GRU, Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, Conv3D, ConvLSTM2D,Conv1D,Activation,LSTM\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import delta\n",
    "from python_speech_features import logfbank\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import glob\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, Bidirectional, TimeDistributed, Conv1D, ZeroPadding1D, GRU\n",
    "from keras.layers import Lambda, Input, Dropout, Masking, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=16000\n",
    "def custom_fft(y, fs):\n",
    "    T = 1.0 / fs\n",
    "    N = y.shape[0]\n",
    "    yf = fft(y)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "    # FFT is simmetrical, so we take just the first half\n",
    "    # FFT is also complex, to we take just the real part (abs)\n",
    "    vals = 2.0/N * np.abs(yf[0:N//2])\n",
    "    return xf, vals\n",
    "\n",
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "# Function for padding smaller wave files\n",
    "def pad_audio(samples):\n",
    "    if len(samples) >= L: return samples\n",
    "    else: return np.pad(samples, pad_width=(L - len(samples), 0), mode='constant', constant_values=(0, 0))\n",
    "    \n",
    "# Function for choppipng larger wave files\n",
    "def chop_audio(samples, L=16000, num=20):\n",
    "    for i in range(num):\n",
    "        beg = np.random.randint(0, len(samples) - L)\n",
    "        yield samples[beg: beg + L]\n",
    "\n",
    "def get_spectrogram(wav):\n",
    "    D = librosa.stft(wav, n_fft=480, hop_length=160,\n",
    "                     win_length=480, window='hamming')\n",
    "    spect, phase = librosa.magphase(D)\n",
    "    return spect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_rate = 16000\n",
    "y_train = []\n",
    "x_train = []\n",
    "mylist1=os.listdir('meld/train/')\n",
    "for file in mylist1:\n",
    "    mylist= os.listdir('meld/train/'+file+\"/\")\n",
    "    for index,y in enumerate(mylist):\n",
    "        # Loadingthe wave files\n",
    "        samples, sample_rate = librosa.load('meld/train/'+file+\"/\"+y,mono=True,sr=16000)\n",
    "        samples=np.array(samples*32768,dtype = \"int16\")\n",
    "        samples = pad_audio(samples)\n",
    "        if len(samples) > 16000:\n",
    "            n_samples = chop_audio(samples)\n",
    "        else: n_samples = [samples]\n",
    "        for samples in n_samples:\n",
    "            # Resampling the wave files\n",
    "            resampled = signal.resample(samples, int(new_sample_rate / sample_rate * samples.shape[0]))\n",
    "            # Log specgrams of wave files\n",
    "            _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "            x_train.append(specgram)\n",
    "            y_train.append(file)\n",
    "# Converting training files to numpy array\n",
    "x_train = np.array(x_train)\n",
    "lb = LabelEncoder()\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the validation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_rate = 16000\n",
    "y_test = []\n",
    "x_test = []\n",
    "mylist1=os.listdir('meld/val/')\n",
    "for file in mylist1:\n",
    "    mylist= os.listdir('meld/val/'+file+\"/\")\n",
    "    for index,y in enumerate(mylist):\n",
    "        samples, sample_rate = librosa.load('meld/val/'+file+\"/\"+y,mono=True,sr=16000)\n",
    "        samples=np.array(samples*32768,dtype = \"int16\")\n",
    "        samples = pad_audio(samples)\n",
    "        if len(samples) > 16000:\n",
    "            n_samples = chop_audio(samples)\n",
    "        else: n_samples = [samples]\n",
    "        for samples in n_samples:\n",
    "            # Resampling wave files\n",
    "            resampled = signal.resample(samples, int(new_sample_rate / sample_rate * samples.shape[0]))\n",
    "            # Log specgram of wave files\n",
    "            _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "            x_test.append(specgram)\n",
    "            y_test.append(file)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np_utils.to_categorical(lb.transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding extra dimension for training 2D CNN\n",
    "x_train = x_train.reshape(tuple(list(x_train.shape) + [1]))\n",
    "x_test = x_test.reshape(tuple(list(x_test.shape) + [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and training 2D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 101, 40, 1)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 101, 40, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 39, 8)        40        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 99, 38, 8)         264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 19, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 49, 19, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 47, 17, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 45, 15, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 22, 7, 16)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 22, 7, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 20, 5, 32)         4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 2, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10, 2, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 108,665\n",
      "Trainable params: 108,151\n",
      "Non-trainable params: 514\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (99, 161, 1)\n",
    "nclass = 5\n",
    "inp = Input(shape=input_shape)\n",
    "norm_inp = BatchNormalization()(inp)\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(norm_inp)\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(32, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Flatten()(img_1)\n",
    "\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(img_1))\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(dense_1))\n",
    "dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "model = models.Model(inputs=inp, outputs=dense_1)\n",
    "opt = optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=opt, loss=losses.categorical_crossentropy,metrics=[\"accuracy\",\"categorical_accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "cnn_history=model.fit(x_train, y_train, batch_size=64,  epochs=10, validation_data=(x_test, y_test),shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and training 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       (None, 99, 81)            0         \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv1D)              (None, 23, 256)           207616    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 23, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 23, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 23, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 23, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 545,925\n",
      "Trainable params: 545,413\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def cnn_lstm(input_dim, output_dim, dropout=0.2, n_layers=1):\n",
    "    dtype = 'float32'\n",
    "    input_data = Input(name='the_input', shape=input_dim, dtype=dtype)\n",
    "    x = Conv1D(filters=256, kernel_size=10, strides=4, name='conv_1')(input_data)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout, name='dropout_1')(x)\n",
    "    x = LSTM(128, activation='relu', return_sequences=True,\n",
    "             dropout=dropout, name='lstm_1')(x)\n",
    "    x = LSTM(128, activation='relu', return_sequences=False,\n",
    "             dropout=dropout, name='lstm_2')(x)\n",
    "    x = Dense(units=64, activation='relu', name='fc')(x)\n",
    "    x = Dropout(dropout, name='dropout_2')(x)\n",
    "    y_pred = Dense(units=output_dim, activation='softmax', name='softmax')(x)\n",
    "    network_model = Model(inputs=input_data, outputs=y_pred)\n",
    "    return network_model\n",
    "\n",
    "input_dim = (99, 81)\n",
    "K.clear_session()\n",
    "model = cnn_lstm(input_dim, 5)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "from keras.optimizers import Adam, SGD\n",
    "sgd = SGD(lr=0.00001, clipnorm=1.0)\n",
    "adam = Adam(lr=1e-4, clipnorm=1.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy','categorical_accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128, epochs=10,\n",
    "                    validation_data=(x_test, y_test),shuffle=True,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model.save('Emotion_Voice_Detection_Model_Filters.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# loading json and creating model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"Emotion_Voice_Detection_Model_Filters.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the loaded model\n",
    "#loaded_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\",\"categorical_accuracy\"])\n",
    "loaded_model.compile(optimizer=opt, loss=\"categorical_crossentropy\",metrics=[\"accuracy\",\"categorical_accuracy\"])\n",
    "score = loaded_model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0788483412869005, 0.6228915662650603, 0.6228915662650603]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830/830 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predicting and checking predictions\n",
    "preds = loaded_model.predict(x_test, \n",
    "                         batch_size=32, \n",
    "                         verbose=1)\n",
    "preds1=preds.argmax(axis=1)\n",
    "abc = preds1.astype(int).flatten()\n",
    "predictions = (lb.inverse_transform((abc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral    830\n",
       "Name: predictedvalues, dtype: int64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preddf = pd.DataFrame({'predictedvalues': predictions})\n",
    "preddf[\"predictedvalues\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
